{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Hyperscaler Training for Legal Reasoning Model\n",
    "\n",
    "This notebook demonstrates how to train the Legal Reasoning Model using SageMaker Hyperscaler on ml.g5.8xlarge instances for optimal price-performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's set up the environment and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure AWS and SageMaker\n",
    "\n",
    "Set up AWS credentials and SageMaker session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set AWS region\n",
    "region = 'us-east-1'  # Change to your preferred region\n",
    "\n",
    "# Create SageMaker session\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "\n",
    "# Get SageMaker execution role\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Set S3 bucket and prefix\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'legal-reasoning-model'\n",
    "\n",
    "print(f\"SageMaker Role ARN: {role}\")\n",
    "print(f\"S3 Bucket: {bucket}\")\n",
    "print(f\"S3 Prefix: {prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration\n",
    "\n",
    "Load the hyperscaler configuration from YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = \"../configs/hyperscaler_config.yaml\"\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Update config with our session values\n",
    "config['aws']['region'] = region\n",
    "config['aws']['s3_bucket'] = bucket\n",
    "\n",
    "# Display configuration\n",
    "print(\"Model Configuration:\")\n",
    "print(json.dumps(config['model'], indent=2))\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "print(json.dumps(config['training'], indent=2))\n",
    "\n",
    "print(\"\\nHyperscaler Configuration:\")\n",
    "print(json.dumps(config['hyperscaler'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data\n",
    "\n",
    "Prepare and upload the training data to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "input_file = \"../data/german/processed/all_examples.jsonl\"\n",
    "output_dir = \"../data/hyperscaler\"\n",
    "language = config['model']['language']\n",
    "\n",
    "# Check if input file exists\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"Input file not found: {input_file}\")\n",
    "    print(\"Please run the data processing script first.\")\n",
    "else:\n",
    "    print(f\"Input file found: {input_file}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    # We'll use the prepare_data_for_hyperscaler.py script\n",
    "    # For demonstration, we'll show the command here\n",
    "    cmd = f\"\"\"python ../scripts/prepare_data_for_hyperscaler.py \\\n",
    "    --input-file {input_file} \\\n",
    "    --output-dir {output_dir} \\\n",
    "    --s3-bucket {bucket} \\\n",
    "    --s3-prefix {prefix}/data \\\n",
    "    --language {language}\"\"\"\n",
    "    \n",
    "    print(\"\\nCommand to prepare data:\")\n",
    "    print(cmd)\n",
    "    \n",
    "    # Note: In a real notebook, you might want to run this command\n",
    "    # using !{cmd} or a subprocess call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Model Parallelism\n",
    "\n",
    "Set up the SageMaker Hyperscaler (model parallelism) configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure distribution for model parallelism\n",
    "distribution = {\n",
    "    \"smdistributed\": {\n",
    "        \"modelparallel\": {\n",
    "            \"enabled\": True,\n",
    "            \"parameters\": {\n",
    "                \"partitions\": config['hyperscaler']['model_parallel_degree'],\n",
    "                \"microbatches\": config['hyperscaler']['microbatches'],\n",
    "                \"optimize\": config['hyperscaler']['optimize'],\n",
    "                \"pipeline_parallel_degree\": config['hyperscaler']['pipeline_parallel_degree'],\n",
    "                \"tensor_parallel_degree\": config['hyperscaler']['tensor_parallel_degree'],\n",
    "                \"ddp\": True,\n",
    "                \"placement_strategy\": config['hyperscaler']['placement_strategy'],\n",
    "                \"activation_checkpointing\": config['hyperscaler']['activation_checkpointing'],\n",
    "                \"prescaled_batch\": config['hyperscaler']['prescaled_batch'],\n",
    "                \"shard_optimizer_state\": config['hyperscaler']['shard_optimizer_state']\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"torch_distributed\": {\n",
    "        \"enabled\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Model Parallelism Configuration:\")\n",
    "print(json.dumps(distribution, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Hyperparameters\n",
    "\n",
    "Configure the training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up hyperparameters\n",
    "hyperparameters = {\n",
    "    # Model configuration\n",
    "    \"model_id\": config['model']['name'],\n",
    "    \"language\": config['model']['language'],\n",
    "    \"max_seq_length\": config['data']['max_seq_length'],\n",
    "    \n",
    "    # Training configuration\n",
    "    \"epochs\": config['training']['num_train_epochs'],\n",
    "    \"per_device_train_batch_size\": config['training']['batch_size'],\n",
    "    \"per_device_eval_batch_size\": config['training']['batch_size'],\n",
    "    \"gradient_accumulation_steps\": config['training']['gradient_accumulation_steps'],\n",
    "    \"learning_rate\": config['training']['learning_rate'],\n",
    "    \"weight_decay\": config['training']['weight_decay'],\n",
    "    \"warmup_steps\": config['training']['warmup_steps'],\n",
    "    \n",
    "    # LoRA configuration\n",
    "    \"use_lora\": str(config['training']['use_lora']).lower(),\n",
    "    \"lora_r\": config['training']['lora_rank'],\n",
    "    \"lora_alpha\": config['training']['lora_alpha'],\n",
    "    \"lora_dropout\": config['training']['lora_dropout'],\n",
    "    \"lora_target_modules\": config['training']['lora_target_modules'],\n",
    "    \n",
    "    # Hyperscaler configuration\n",
    "    \"model_parallel_degree\": config['hyperscaler']['model_parallel_degree'],\n",
    "    \"ddp_dist_backend\": \"nccl\",\n",
    "    \"fp16\": str(config['training']['fp16']).lower(),\n",
    "    \"bf16\": str(config['training']['bf16']).lower(),\n",
    "    \n",
    "    # Optimization\n",
    "    \"deepspeed_config\": \"ds_z3_config.json\",  # Will be created in the entry point\n",
    "    \"torch_distributed\": \"true\",\n",
    "    \n",
    "    # Checkpointing\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 500,\n",
    "    \"save_total_limit\": 2,\n",
    "    \n",
    "    # Evaluation\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"eval_steps\": 500,\n",
    "    \"logging_steps\": 100,\n",
    "    \n",
    "    # Output\n",
    "    \"output_dir\": \"/opt/ml/model\"\n",
    "}\n",
    "\n",
    "print(\"Training Hyperparameters:\")\n",
    "print(json.dumps(hyperparameters, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker Estimator\n",
    "\n",
    "Create a SageMaker HuggingFace estimator with Hyperscaler configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training job name\n",
    "import datetime\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "job_name = f\"legal-reasoning-{config['model']['language']}-{timestamp}\"\n",
    "\n",
    "# Set up instance configuration\n",
    "instance_type = config['aws']['instance_type']\n",
    "instance_count = config['aws']['instance_count']\n",
    "use_spot = config['aws']['use_spot']\n",
    "max_wait = config['aws']['max_wait'] if use_spot else None\n",
    "max_run = config['aws']['max_run']\n",
    "\n",
    "# Set up output path\n",
    "output_path = f\"s3://{bucket}/{prefix}/model\"\n",
    "checkpoint_s3_uri = f\"s3://{bucket}/{prefix}/checkpoints\" if use_spot else None\n",
    "\n",
    "# Create HuggingFace estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train_hyperscaler.py\",  # Custom training script with model parallelism\n",
    "    source_dir=\"../src/training\",         # Directory containing the training code\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    "    role=role,\n",
    "    transformers_version=\"4.28.1\",\n",
    "    pytorch_version=\"2.0.0\",\n",
    "    py_version=\"py310\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    distribution=distribution,\n",
    "    use_spot_instances=use_spot,\n",
    "    max_wait=max_wait,\n",
    "    max_run=max_run,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    output_path=output_path,\n",
    "    base_job_name=job_name,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "print(f\"Job Name: {job_name}\")\n",
    "print(f\"Instance Type: {instance_type}\")\n",
    "print(f\"Using Spot Instances: {use_spot}\")\n",
    "print(f\"Output Path: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
