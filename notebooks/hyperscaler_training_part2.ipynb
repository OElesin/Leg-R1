{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Hyperscaler Training for Legal Reasoning Model (Part 2)\n",
    "\n",
    "This notebook demonstrates how to train the Legal Reasoning Model using SageMaker Hyperscaler on ml.g5.8xlarge instances for optimal price-performance.\n",
    "\n",
    "## Part 2: Model Parallelism Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First, let's import the necessary libraries and load the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# Load configuration\n",
    "config_path = \"../configs/hyperscaler_config.yaml\"\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Set AWS region\n",
    "region = 'us-east-1'  # Change to your preferred region\n",
    "\n",
    "# Create SageMaker session\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'legal-reasoning-model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Model Parallelism\n",
    "\n",
    "Set up the SageMaker Hyperscaler (model parallelism) configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure distribution for model parallelism\n",
    "distribution = {\n",
    "    \"smdistributed\": {\n",
    "        \"modelparallel\": {\n",
    "            \"enabled\": True,\n",
    "            \"parameters\": {\n",
    "                \"partitions\": config['hyperscaler']['model_parallel_degree'],\n",
    "                \"microbatches\": config['hyperscaler']['microbatches'],\n",
    "                \"optimize\": config['hyperscaler']['optimize'],\n",
    "                \"pipeline_parallel_degree\": config['hyperscaler']['pipeline_parallel_degree'],\n",
    "                \"tensor_parallel_degree\": config['hyperscaler']['tensor_parallel_degree'],\n",
    "                \"ddp\": True,\n",
    "                \"placement_strategy\": config['hyperscaler']['placement_strategy'],\n",
    "                \"activation_checkpointing\": config['hyperscaler']['activation_checkpointing'],\n",
    "                \"prescaled_batch\": config['hyperscaler']['prescaled_batch'],\n",
    "                \"shard_optimizer_state\": config['hyperscaler']['shard_optimizer_state']\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"torch_distributed\": {\n",
    "        \"enabled\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Model Parallelism Configuration:\")\n",
    "print(json.dumps(distribution, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Hyperparameters\n",
    "\n",
    "Configure the training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up hyperparameters\n",
    "hyperparameters = {\n",
    "    # Model configuration\n",
    "    \"model_id\": config['model']['name'],\n",
    "    \"language\": config['model']['language'],\n",
    "    \"max_seq_length\": config['data']['max_seq_length'],\n",
    "    \n",
    "    # Training configuration\n",
    "    \"epochs\": config['training']['num_train_epochs'],\n",
    "    \"per_device_train_batch_size\": config['training']['batch_size'],\n",
    "    \"per_device_eval_batch_size\": config['training']['batch_size'],\n",
    "    \"gradient_accumulation_steps\": config['training']['gradient_accumulation_steps'],\n",
    "    \"learning_rate\": config['training']['learning_rate'],\n",
    "    \"weight_decay\": config['training']['weight_decay'],\n",
    "    \"warmup_steps\": config['training']['warmup_steps'],\n",
    "    \n",
    "    # LoRA configuration\n",
    "    \"use_lora\": str(config['training']['use_lora']).lower(),\n",
    "    \"lora_r\": config['training']['lora_rank'],\n",
    "    \"lora_alpha\": config['training']['lora_alpha'],\n",
    "    \"lora_dropout\": config['training']['lora_dropout'],\n",
    "    \"lora_target_modules\": config['training']['lora_target_modules'],\n",
    "    \n",
    "    # Hyperscaler configuration\n",
    "    \"model_parallel_degree\": config['hyperscaler']['model_parallel_degree'],\n",
    "    \"ddp_dist_backend\": \"nccl\",\n",
    "    \"fp16\": str(config['training']['fp16']).lower(),\n",
    "    \"bf16\": str(config['training']['bf16']).lower(),\n",
    "    \n",
    "    # Optimization\n",
    "    \"deepspeed_config\": \"ds_z3_config.json\",  # Will be created in the entry point\n",
    "    \"torch_distributed\": \"true\",\n",
    "    \n",
    "    # Checkpointing\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 500,\n",
    "    \"save_total_limit\": 2,\n",
    "    \n",
    "    # Evaluation\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"eval_steps\": 500,\n",
    "    \"logging_steps\": 100,\n",
    "    \n",
    "    # Output\n",
    "    \"output_dir\": \"/opt/ml/model\"\n",
    "}\n",
    "\n",
    "print(\"Training Hyperparameters:\")\n",
    "print(json.dumps(hyperparameters, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
