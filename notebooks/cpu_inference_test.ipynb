{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing CPU Inference for Legal Reasoning Model\n",
    "\n",
    "This notebook demonstrates how to test the CPU-optimized Legal Reasoning Model locally before deploying to SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Add the src directory to the path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import CPU inference code\n",
    "from inference.cpu_inference import model_fn, predict_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model\n",
    "\n",
    "First, let's load the optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the optimized model\n",
    "model_dir = \"../models/optimized\"\n",
    "\n",
    "# Load the model\n",
    "print(\"Loading model...\")\n",
    "start_time = time.time()\n",
    "model, tokenizer = model_fn(model_dir)\n",
    "load_time = time.time() - start_time\n",
    "print(f\"Model loaded in {load_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Inference\n",
    "\n",
    "Now, let's test inference with a sample legal document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample document\n",
    "sample_path = \"../data/german/processed/IX ZB 72_08.json\"\n",
    "with open(sample_path, 'r', encoding='utf-8') as f:\n",
    "    sample_doc = json.load(f)\n",
    "\n",
    "# Extract text\n",
    "sample_text = sample_doc['full_text']\n",
    "\n",
    "# Truncate text to avoid token limits\n",
    "max_chars = 5000\n",
    "if len(sample_text) > max_chars:\n",
    "    sample_text = sample_text[:max_chars] + \"...\"\n",
    "\n",
    "print(f\"Sample text length: {len(sample_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input data\n",
    "input_data = {\n",
    "    \"text\": sample_text,\n",
    "    \"task\": \"summarization\",\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"language\": \"de\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "print(\"Running inference...\")\n",
    "start_time = time.time()\n",
    "result = predict_fn(input_data, (model, tokenizer))\n",
    "inference_time = time.time() - start_time\n",
    "print(f\"Inference completed in {inference_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the result\n",
    "print(\"Generated response:\")\n",
    "print(result[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Different Tasks\n",
    "\n",
    "Let's benchmark the model on different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tasks to benchmark\n",
    "tasks = [\"classification\", \"summarization\", \"case_analysis\", \"statute_interpretation\"]\n",
    "\n",
    "# Benchmark results\n",
    "benchmark_results = {}\n",
    "\n",
    "for task in tasks:\n",
    "    print(f\"\\nBenchmarking task: {task}\")\n",
    "    \n",
    "    # Prepare input data\n",
    "    input_data = {\n",
    "        \"text\": sample_text,\n",
    "        \"task\": task,\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"language\": \"de\"\n",
    "    }\n",
    "    \n",
    "    # Run inference\n",
    "    start_time = time.time()\n",
    "    result = predict_fn(input_data, (model, tokenizer))\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Store result\n",
    "    benchmark_results[task] = {\n",
    "        \"time\": inference_time,\n",
    "        \"tokens_per_second\": 256 / inference_time\n",
    "    }\n",
    "    \n",
    "    print(f\"Inference time: {inference_time:.2f} seconds\")\n",
    "    print(f\"Tokens per second: {256 / inference_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot benchmark results\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Task': list(benchmark_results.keys()),\n",
    "    'Inference Time (s)': [benchmark_results[task]['time'] for task in benchmark_results],\n",
    "    'Tokens per Second': [benchmark_results[task]['tokens_per_second'] for task in benchmark_results]\n",
    "})\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Inference time\n",
    "ax1.bar(df['Task'], df['Inference Time (s)'])\n",
    "ax1.set_title('Inference Time by Task')\n",
    "ax1.set_ylabel('Time (seconds)')\n",
    "ax1.set_xlabel('Task')\n",
    "\n",
    "# Tokens per second\n",
    "ax2.bar(df['Task'], df['Tokens per Second'])\n",
    "ax2.set_title('Tokens per Second by Task')\n",
    "ax2.set_ylabel('Tokens per Second')\n",
    "ax2.set_xlabel('Task')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Usage Analysis\n",
    "\n",
    "Let's analyze the memory usage of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model size\n",
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_all_mb\n",
    "\n",
    "model_size_mb = get_model_size(model)\n",
    "print(f\"Model size in memory: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated how to test the CPU-optimized Legal Reasoning Model locally. The model can now be deployed to a CPU-based SageMaker endpoint using the `deploy_cpu_model.py` script."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
